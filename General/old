import numpy as np
from layer import *
from math_for_cnn import *

epsilon = 0.00001

class CNN(object):
    def __init__(self, layers, batch_size = 20, num_input_channels = 3, height = 28, width = 28):
        self.layers=layers
        self.batch_size = batch_size

        #INPUT
        self.X = [None]*(len(layers)+1)
        self.im2rowx = [None]*len(layers)
        #WEIGHTS (even if they are empty)
        self.W = [None]*(len(layers))
        self.E_grad2 = [None]*(len(layers))
        self.E_x2 = [None]*(len(layers))
        self.v = [None]*(len(layers))

        self.W_mean = [None]*(len(layers))
        self.W_variance = [None]*(len(layers))

        last_n = num_input_channels
        last_h = height
        last_w = width
        for i in range(len(layers)):
            layer = self.layers[i]
            self.v[i] = 0
            if(layer.function is "convolution"):
                n, d, size = layer.num_filters, last_n, layer.kernel_size
                pad, stride = layer.pad, layer.stride
                #attempt at xavier initialization
                self.W[i] = np.random.randn(n, d, size, size) / np.sqrt(d*size*size)
                #for adadelta
                self.E_grad2[i] = np.zeros((n, last_n, size, size))
                self.E_x2[i] = np.zeros((n, last_n, size, size))
                #when done:
                last_n = n
                last_h = int((last_h+pad*2-size)/stride + 1)
                last_w = int((last_w+pad*2-size)/stride + 1)

            elif(layer.function is "BN"):
                #init gamma to one, beta to zeros
                #subscript zero = gamma, subscript one = beta
                self.W[i] = np.ones((2, last_n, last_h, last_w))
                self.W[i][0] = np.zeros((last_n, last_h, last_w))
                #for adadelta
                self.E_grad2[i] = np.zeros((2, last_n, last_h, last_w))
                self.E_x2[i] = np.zeros((2, last_n, last_h, last_w))

            elif(layer.function is "maxpool"):
                pass
                #TODO

    def forward(self, input_matrix):
        self.X[0] = input_matrix

        for i in range(len(self.layers)):
            if(self.layers[i].function is "convolution"):
                #variables
                in_R, in_D, in_H, in_W = self.X[i].shape
                num_filters, kernel_D, kernel_H, kernel_W = self.W[i].shape
                out_R = in_R
                out_D = self.W[i].shape[0]
                out_H = int((in_H+self.layers[i].pad*2-self.layers[i].kernel_size)/self.layers[i].stride + 1)
                out_W = int((in_W+self.layers[i].pad*2-self.layers[i].kernel_size)/self.layers[i].stride + 1)

                #math
                self.im2rowx[i] = im2row(self.X[i], size = self.layers[i].kernel_size, stride = self.layers[i].stride, pad = self.layers[i].pad)
                y = np.dot(self.im2rowx[i], self.W[i].T.reshape(( kernel_D*kernel_H*kernel_W , num_filters )))
                self.X[i+1] = y.reshape((out_R, out_H, out_W, out_D)).transpose(0, 3, 1, 2)

            elif(self.layers[i].function is "ReLU"):
                self.X[i+1] = relu(self.X[i])

            elif(self.layers[i].function is "tanh"):
                self.X[i+1] = tanh(self.X[i])

            elif(self.layers[i].function is "BN"):
                x = self.X[i]
                gamma = self.W[i][0]
                beta = self.W[i][1]

                mean = np.mean(x, axis = 0)
                variance = np.mean((x-mean)**2, axis = 0)

                xhat = (x-mean) / np.sqrt(variance + epsilon)
                self.X[i+1] = gamma * xhat + beta

            elif(self.layers[i].function is "maxpool"):
                pass
                ######TODO

        return softmax(self.X[len(self.layers)])

    def backprop(self, prediction, actual_value):
        length = len(self.layers)
        dJdW = [None]*(length)

        #softmax
        delta = prediction - actual_value

        #loop for the rest of the layers
        for i in range((length-1), -1, -1):
            if(self.layers[i].function is "convolution"):
                delta_reshaped = delta.transpose(0,2,3,1).reshape(delta.shape[0]*delta.shape[2]*delta.shape[3], delta.shape[1])
                dJdW[i] = np.dot(self.im2rowx[i].T, delta_reshaped).T.reshape(self.W[i].shape)
                delta = row2im(mat = delta, W = self.W[i], delta_shape = self.X[i].shape, stride = self.layers[i].stride, pad = self.layers[i].pad)

                ##TESTING BATCH SIZE DIVISION
                #batch_size = self.X[i].shape[0]
                #dJdW[i] /= batch_size
            elif(self.layers[i].function is "ReLU"):
                delta = delta * relu_prime(self.X[i])

            elif(self.layers[i].function is "tanh"):
                delta = delta * tanh_prime(self.X[i])

            elif(self.layers[i].function is "BN"):
                x = self.X[i]
                dy = delta
                gamma = self.W[i][0]
                beta = self.W[i][1]
                R = x.shape[0]

                mean = np.mean(x, axis = 0)
                variance = np.mean((x-mean)**2, axis = 0)
                dxdgamma = np.sum((x - mean) / np.sqrt(variance + epsilon) * dy, axis=0)
                dxdbeta = np.sum(dy, axis=0)
                dx1dx0 = gamma / R / np.sqrt(variance + epsilon) * (R * dy - np.sum(dy, axis=0) - (x - mean) / (variance + epsilon) * np.sum(dy * (x - mean), axis=0))

                dJdW[i] = np.empty((2, dxdgamma.shape[0], dxdgamma.shape[1], dxdgamma.shape[2]))
                dJdW[i][0] = dxdgamma
                dJdW[i][1] = dxdbeta
                delta = delta * dx1dx0

            elif(self.layers[i].function is "maxpool"):
                pass
                ######TODO

            ##TODO calculating 1 useless thing, check notes #TODO

        return dJdW
